import sys,os
import numpy as np
import tensorflow as tf

def load_vocab(filename):
    d = {}
    reverse_d = {}
    with open(filename) as f:
        for idx, line in enumerate(f):
            d[line.strip()] = idx
            reverse_d[idx] = line.strip()
    return d, reverse_d

def get_trimmed_glove_vectors(filename):
    #return matrix of embeddings
    #with np.load(filename) as data:
    data = np.load(filename)
    return data["embeddings"]

class Config():
    def __init__(self, use_emb=0):
        #self.use_pretrained = False
        self.use_pretrained = True
        if use_emb == -1:
            self.use_pretrained = False
        self.vocabfile = "data/flight/words.txt"
        self.tagfile = "data/flight/output_slot.txt"
        self.intentfile = "data/flight/intent.txt"
        self.inputtagfile = "data/flight/input_slot.txt"
        #self.vocabfile = "data/music.vocab"
        #self.tagfile = "data/musictag.vocab"
        self.trainfile = "data/flight/train.txt"
        self.devfile = "data/flight/dev.txt"
        self.testfile = "data/flight/test.txt"
        #self.trainfile = "data/music.train"
        #self.devfile = "data/music.dev"
        #self.testfile = "data/music.test"
        self.nepochs = 20
        #self.nepochs = 50
        self.log_file = "log/logger"
        self.batch_size = 10
        self.lr = 0.001
        self.intent_weight = 0.5
        self.dropout = 0.5
        self.lr_decay = 0.9
        self.nepoch_no_imprv  = 10
        self.vocab_words, self.idx2vocab = load_vocab(self.vocabfile)
        self.vocab_tags, self.idx2tag = load_vocab(self.tagfile)
        self.vocab_intents, self.idx2intent = load_vocab(self.intentfile)
        self.vocab_inputtags, self.idx2inputintent = \
                load_vocab(self.inputtagfile)
        self.dir_output = "results/test/"
        self.dir_model = self.dir_output + "model.weights/"
        self.vocab_size = len(self.vocab_words)
        self.tag_size = len(self.vocab_tags)
        self.num_intents = len(self.vocab_intents)
        self.inputtag_size = len(self.vocab_inputtags)
        self.mlp_num_units = 128
        self.dim_word = 200
        self.hidden_size_lstm = 200
        self.filename_trimmed = "data/flight/word.{}d.trimmed.npz".format(self.dim_word)
        #self.filename_trimmed = "data/glove.6B.{}d.trimmed.npz".format(300)
        self.embeddings = (get_trimmed_glove_vectors(self.filename_trimmed) if self.use_pretrained else None)
        self.lr_method = "adam"
        self.clip = -1 # if negative, no clipping

        if not os.path.exists(self.dir_output):
            os.makedirs(self.dir_output)

if __name__ == "__main__":
    if len(sys.argv) >= 2 and sys.argv[1] == "emb":
        config = Config(-1)
        embeddings = np.random.rand(len(config.vocab_words), config.dim_word)
        fin = open("/home/work/chenyanfeng/dict/tencent.emb.cache", "r")
        #fin = open("dict/flight.input.word.emb", "r")
        for line in fin:
            lin = line.strip().split(" ")
            if lin[0] in config.vocab_words:
                idx = config.vocab_words[lin[0]]
                emb = [float(x) for x in lin[1:]]
                embeddings[idx] = np.asarray(emb)
        fin.close()
        np.savez_compressed(config.filename_trimmed, embeddings=embeddings)
